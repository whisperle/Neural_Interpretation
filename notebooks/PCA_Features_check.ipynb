{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 19:52:57.326280: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-24 19:52:57.869284: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-24 19:52:58.136098: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-24 19:52:58.136124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-24 19:52:58.147232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-24 19:52:58.194071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 19:53:00.048231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling NSD webdataset data...\n",
      "PID of this process = 840560\n",
      "device: cpu\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/scratch/cl6707/Shared_Datasets/NSD_MindEye\"\n",
    "nsd_root = \"/scratch/cl6707/Projects/neuro_interp/data/NSD/\"\n",
    "stim_root = nsd_root + \"nsddata_stimuli/stimuli/nsd/\"\n",
    "beta_root = nsd_root + \"nsddata_betas/ppdata/\"\n",
    "mask_root = nsd_root + \"nsddata/ppdata/\"\n",
    "nsd_mindroot = '/scratch/cl6707/Shared_Datasets/NSD_MindEye'\n",
    "subj = 1\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import sys\n",
    "sys.path.append('/scratch/ne2213/projects/tmp_packages')\n",
    "sys.path.append('/scratch/ne2213/projects/tmp_packages/')\n",
    "sys.path.append('/scratch/cl6707/Projects/neuro_interp/data/NSD/nsd')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)\n",
    "from src.file_utility import load_mask_from_nii, view_data\n",
    "\n",
    "%matplotlib inline\n",
    "fpX = np.float32\n",
    "import src.numpy_utility as pnu\n",
    "from src.file_utility import load_mask_from_nii\n",
    "from nilearn.decoding import SpaceNetRegressor\n",
    "import nibabel as nib\n",
    "\n",
    "subj =1\n",
    "voxel_roi_full  = load_mask_from_nii(mask_root + \"subj%02d/func1pt8mm/roi/prf-visualrois.nii.gz\"%subj)\n",
    "voxel_roi_full.shape\n",
    "\n",
    "# from NSDAccess import NSDAccess\n",
    "# nsd = NSDAccess('/scratch/cl6707/Projects/neuro_interp/data/NSD/')\n",
    "\n",
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "    \n",
    "    \n",
    "print('Pulling NSD webdataset data...')\n",
    "# Multi-GPU config #\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(split_batches=False,mixed_precision='fp16')  \n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print = accelerator.print # only print if local_rank=0\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "local_rank = accelerator.state.local_process_index\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "\n",
    "\n",
    "\n",
    "# train_url = \"{\" + f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar,\" + f\"{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\" + \"}\"\n",
    "# val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "# print(train_url,\"\\n\",val_url)\n",
    "# meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "# num_train = 8559 + 300\n",
    "# num_val = 982\n",
    "# batch_size = 1\n",
    "# print('Prepping train and validation dataloaders...')\n",
    "\n",
    "# train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "#     batch_size,'images',\n",
    "#     num_devices=num_devices,\n",
    "#     num_workers=num_workers,\n",
    "#     train_url=train_url,\n",
    "#     val_url=val_url,\n",
    "#     meta_url=meta_url,\n",
    "#     num_train=num_train,\n",
    "#     num_val=num_val,\n",
    "#     val_batch_size=1,\n",
    "#     cache_dir=data_path, #\"/tmp/wds-cache\",\n",
    "#     seed=seed,\n",
    "#     voxels_key='nsdgeneral.npy', # 'nsdgeneral.npy' (1d), 'wholebrain_3d.npy'(3d)\n",
    "#     to_tuple=[\"voxels\", \"images\", \"coco\",\"trial\"],\n",
    "#     local_rank=local_rank,\n",
    "#     world_size=world_size,\n",
    "#     reloading=True,\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdgeneral_affine = nib.load('/scratch/cl6707/Projects/neuro_interp/data/NSD/nsddata/ppdata/subj01/func1pt8mm/roi/nsdgeneral.nii.gz').affine\n",
    "nsdgeneral =  nib.load('/scratch/cl6707/Projects/neuro_interp/data/NSD/nsddata/ppdata/subj01/func1pt8mm/roi/nsdgeneral.nii.gz')\n",
    "nsdgeneral_roi_mask = (nsdgeneral.get_fdata()==1).astype(np.float32)\n",
    "nsdgeneral = nib.Nifti1Image(nsdgeneral_roi_mask, nsdgeneral_affine)\n",
    "anat_img = '/scratch/cl6707/Projects/neuro_interp/data/NSD/nsddata/ppdata/subj01/func1pt8mm/T1_to_func1pt8mm.nii.gz'\n",
    "things = np.load(nsd_mindroot + '/subj%02d_things.npy'%subj,allow_pickle=True)\n",
    "things_all = np.concatenate(things, axis=0)\n",
    "unique_things = np.unique(things_all)\n",
    "things_val_mapping = {k:i for i,k in enumerate(unique_things)}\n",
    "val_things_mapping = {i:k for i,k in enumerate(unique_things)}\n",
    "\n",
    "\n",
    "def reconstruct_volume_corrected(vol_shape, binary_mask, data_vol, order='C'):\n",
    "    \n",
    "    view_vol = np.ones(np.prod(vol_shape), dtype=np.float32) * np.nan\n",
    "    \n",
    "    idx_mask = np.where(binary_mask)[0]\n",
    "    \n",
    "    view_vol[idx_mask] = data_vol\n",
    "    return np.nan_to_num(view_vol.reshape(vol_shape, order=order))\n",
    "\n",
    "file_path = '/scratch/ne2213/projects/IVP/Neural_Interpretation-main/notebooks/new_dataloader_50.pkl'\n",
    "file_path_val = '/scratch/ne2213/projects/IVP/Neural_Interpretation-main/notebooks/feature_files/val_dataloader_50.pkl'\n",
    "data_loader_pca = pickle.load(open(file_path, 'rb'))\n",
    "data_loader_pca_val = pickle.load(open(file_path_val, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3defd36cfe744061b6188a878b8b429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2db662f7d304b349975a71640ded07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_iter = iter(data_loader_pca)\n",
    "val_iter = iter(data_loader_pca_val)\n",
    "X_train_all = []\n",
    "y_train_all = []\n",
    "X_val_all = []\n",
    "y_val_all = []\n",
    "\n",
    "for i in tqdm(range(num_train)):\n",
    "    voxels, images, coco, trial, feature = next(train_iter)\n",
    "    voxels = voxels[0].cpu().numpy().mean(axis=0)\n",
    "    # convert voxels to 3d volume\n",
    "    voxels = reconstruct_volume_corrected(nsdgeneral_roi_mask.shape, nsdgeneral_roi_mask.flatten(), voxels)\n",
    "    nii_voxels = nib.Nifti1Image(voxels, nsdgeneral_affine)\n",
    "    labels = things[trial]\n",
    "    # for label in labels:\n",
    "    #     X_train_all.append(nii_voxels)\n",
    "    #     y_train_all.append(things_val_mapping[label])\n",
    "    if len(labels)>=1 and len(labels)<=3:\n",
    "        X_train_all.append(feature.flatten().cpu().numpy())\n",
    "        y_train_all.append(things_val_mapping[labels[0]])\n",
    "\n",
    "for i in tqdm(range(num_val)):\n",
    "    voxels, images, coco, trial, feature = next(val_iter)\n",
    "    voxels = voxels[0].cpu().numpy().mean(axis=0)\n",
    "    # convert voxels to 3d volume\n",
    "    voxels = reconstruct_volume_corrected(nsdgeneral_roi_mask.shape, nsdgeneral_roi_mask.flatten(), voxels)\n",
    "    nii_voxels = nib.Nifti1Image(voxels, nsdgeneral_affine)\n",
    "    labels = things[trial]\n",
    "    # for label in labels:\n",
    "    #     X_val_all.append(nii_voxels)\n",
    "    #     y_val_all.append(things_val_mapping[label])\n",
    "    if len(labels)>=1 and len(labels)<=3:\n",
    "        X_val_all.append(feature.flatten().cpu().numpy())\n",
    "        y_val_all.append(things_val_mapping[labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels, images, coco, trial = next(iter(train_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27750, 256, 256, 3)\n",
      "(27750, 15724)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File('/scratch/cl6707/Shared_Datasets/NSD_MindEye/subj01_nsdgeneral.hdf5', 'r') as f:\n",
    "    for key in f.keys():\n",
    "        print(f[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/metadata_subj02.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'totals': {'train': 8559, 'val': 300, 'test': 982}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "subj=2\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "print(meta_url)\n",
    "meta_file = json.load(open(meta_url, 'r'))\n",
    "meta_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totals': {'train': 8559, 'val': 300, 'test': 982}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cl6707/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/train/train_subj02_{0..17}.tar,/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/val/val_subj02_0.tar} \n",
      " /scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/test/test_subj02_{0..1}.tar\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 1\n",
      "batch_size 1\n",
      "num_workers 1\n",
      "num_batches 8859\n",
      "num_worker_batches 8859\n",
      "Reloading Training data without shuffling!\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 982\n",
      "val_batch_size 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89743fb08d4544d59e86999d30e596ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b51cbd6bb0438ea310b1dcf9d719af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8ac00378e5424c92b5b191b45cce18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485cb6c165fe4ec3a08a482c6708930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/train/train_subj05_{0..17}.tar,/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/val/val_subj05_0.tar} \n",
      " /scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/test/test_subj05_{0..1}.tar\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 1\n",
      "batch_size 1\n",
      "num_workers 1\n",
      "num_batches 8859\n",
      "num_worker_batches 8859\n",
      "Reloading Training data without shuffling!\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 982\n",
      "val_batch_size 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c42ac26393c45dea59fe4852bf99000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1549536fbc6a467f9d6dbd78f70a7201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc4c1ca8fac44d4b8a0743ec1613b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d53828770c4d8bbf4ffbe8a1218095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/train/train_subj07_{0..17}.tar,/scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/val/val_subj07_0.tar} \n",
      " /scratch/cl6707/Shared_Datasets/NSD_MindEye/webdataset_avg_split/test/test_subj07_{0..1}.tar\n",
      "Prepping train and validation dataloaders...\n",
      "Getting dataloaders...\n",
      "\n",
      "num_train 8859\n",
      "global_batch_size 1\n",
      "batch_size 1\n",
      "num_workers 1\n",
      "num_batches 8859\n",
      "num_worker_batches 8859\n",
      "Reloading Training data without shuffling!\n",
      "\n",
      "num_val 982\n",
      "val_num_batches 982\n",
      "val_batch_size 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fb5420690843f4a0bde0e769690ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bf56c98c4e451cb27ecfb95bc25f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4850358e69394bd38da12184228e9a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c436e28e718a4ca9b70d05c2f0be3ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resnet50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "#get the last layer of the resnet50\n",
    "resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "resnet50.eval().to(device)\n",
    "for subj in [2,5,7]:\n",
    "    train_url = \"{\" + f\"{data_path}/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar,\" + f\"{data_path}/webdataset_avg_split/val/val_subj0{subj}_0.tar\" + \"}\"\n",
    "    val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "    print(train_url,\"\\n\",val_url)\n",
    "    meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "    num_train = 8559 + 300\n",
    "    num_val = 982\n",
    "    batch_size = 1\n",
    "    print('Prepping train and validation dataloaders...')\n",
    "\n",
    "    train_dl, val_dl, num_train, num_val = utils.get_dataloaders(\n",
    "        batch_size,'images',\n",
    "        num_devices=num_devices,\n",
    "        num_workers=num_workers,\n",
    "        train_url=train_url,\n",
    "        val_url=val_url,\n",
    "        meta_url=meta_url,\n",
    "        num_train=num_train,\n",
    "        num_val=num_val,\n",
    "        val_batch_size=1,\n",
    "        cache_dir=data_path, #\"/tmp/wds-cache\",\n",
    "        seed=seed,\n",
    "        voxels_key='nsdgeneral.npy', # 'nsdgeneral.npy' (1d), 'wholebrain_3d.npy'(3d)\n",
    "        to_tuple=[\"voxels\", \"images\", \"coco\",\"trial\"],\n",
    "        local_rank=local_rank,\n",
    "        world_size=world_size,\n",
    "        reloading=True,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    voxels_train_all = []\n",
    "    image_train_all = []\n",
    "    coco_train_all = []\n",
    "    trial_train_all = []\n",
    "    feature_train_all = []\n",
    "    train_iter = iter(train_dl)\n",
    "    for _ in tqdm(range(num_train)):\n",
    "        voxels, images, coco, trial =  next(train_iter)\n",
    "        voxels_train_all.append(voxels.cpu().numpy())\n",
    "        image_train_all.append(images.cpu().numpy())\n",
    "        coco_train_all.append(coco.cpu().numpy())\n",
    "        trial_train_all.append(trial.cpu().numpy())\n",
    "\n",
    "\n",
    "    voxels_val_all = []\n",
    "    image_val_all = []\n",
    "    coco_val_all = []\n",
    "    trial_val_all = []\n",
    "    feature_val_all = []\n",
    "    val_iter = iter(val_dl)\n",
    "    for _ in tqdm(range(num_val)):\n",
    "        voxels, images, coco, trial =  next(val_iter)\n",
    "        voxels_val_all.append(voxels.cpu().numpy())\n",
    "        image_val_all.append(images.cpu().numpy())\n",
    "        coco_val_all.append(coco.cpu().numpy())\n",
    "        trial_val_all.append(trial.cpu().numpy())\n",
    "\n",
    "        \n",
    "    voxels_train_all = np.concatenate(voxels_train_all, axis=0)\n",
    "    image_train_all = np.concatenate(image_train_all, axis=0)\n",
    "    coco_train_all = np.concatenate(coco_train_all, axis=0)\n",
    "    trial_train_all = np.concatenate(trial_train_all, axis=0)\n",
    "\n",
    "\n",
    "    voxels_val_all = np.concatenate(voxels_val_all, axis=0)\n",
    "    image_val_all = np.concatenate(image_val_all, axis=0)\n",
    "    coco_val_all = np.concatenate(coco_val_all, axis=0)\n",
    "    trial_val_all = np.concatenate(trial_val_all, axis=0)\n",
    "\n",
    "    for image in tqdm(image_train_all):\n",
    "        image = torch.from_numpy(image).to(device).unsqueeze(0)\n",
    "        feature_train_all.append(resnet50(image).cpu().detach().numpy())\n",
    "    for image in tqdm(image_val_all):\n",
    "        image = torch.from_numpy(image).to(device).unsqueeze(0)\n",
    "        feature_val_all.append(resnet50(image).cpu().detach().numpy())\n",
    "\n",
    "    feature_train_all = np.concatenate(feature_train_all, axis=0)\n",
    "    feature_val_all = np.concatenate(feature_val_all, axis=0)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca50= PCA(n_components=50)\n",
    "    feature_all = np.concatenate([feature_train_all, feature_val_all], axis=0).squeeze()\n",
    "    pca50.fit(feature_all)\n",
    "    pca_feature_train_all = pca50.transform(feature_train_all.squeeze())\n",
    "    pca_feature_val_all = pca50.transform(feature_val_all.squeeze())\n",
    "\n",
    "    import h5py\n",
    "    nsd_path = '/scratch/cl6707/Shared_Datasets/NSD_MindEye/'\n",
    "    # Save the data to a hdf5 file\n",
    "\n",
    "    with h5py.File(nsd_path + 'train_subj%02d_pca50.hdf5'%subj, 'w') as f:\n",
    "        f.create_dataset('voxels', data=voxels_train_all)\n",
    "        f.create_dataset('images', data=image_train_all)\n",
    "        f.create_dataset('coco', data=coco_train_all)\n",
    "        f.create_dataset('trial', data=trial_train_all)\n",
    "        f.create_dataset('res_feature', data=feature_train_all)\n",
    "        f.create_dataset('pca_feature', data=pca_feature_train_all)\n",
    "\n",
    "    with h5py.File(nsd_path + 'val_subj%02d_pca50.hdf5'%subj, 'w') as f:\n",
    "        f.create_dataset('voxels', data=voxels_val_all)\n",
    "        f.create_dataset('images', data=image_val_all)\n",
    "        f.create_dataset('coco', data=coco_val_all)\n",
    "        f.create_dataset('trial', data=trial_val_all)\n",
    "        f.create_dataset('res_feature', data=feature_val_all)\n",
    "        f.create_dataset('pca_feature', data=pca_feature_val_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivp",
   "language": "python",
   "name": "ivp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
